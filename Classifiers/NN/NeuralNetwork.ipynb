{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "script_path = os.path.join(os.getcwd(), \"../../Scripts/\")\n",
    "sys.path.append(script_path)\n",
    "import data_generator as dgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Примерная функция для нормализации\n",
    "def normalize(signal):\n",
    "    return (signal - np.mean(signal)) / np.std(signal)\n",
    "\n",
    "# Класс для подготовки датасета\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.fixed_length = 5000  # Пример длины для padding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Берем данные пациента\n",
    "        ecg_signal = self.data[idx]\n",
    "        \n",
    "        # Применяем нормализацию к каждому каналу\n",
    "        ecg_signal = np.array([normalize(ch) for ch in ecg_signal])\n",
    "        \n",
    "        # Padding/Truncation до фиксированной длины\n",
    "        ecg_signal = self._fix_length(ecg_signal)\n",
    "        \n",
    "        # Преобразование в torch.tensor\n",
    "        ecg_signal = torch.tensor(ecg_signal, dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        \n",
    "        return ecg_signal, label\n",
    "    \n",
    "    def _fix_length(self, ecg_signal):\n",
    "        # Применяем padding или обрезание\n",
    "        if ecg_signal.shape[1] < self.fixed_length:\n",
    "            pad_size = self.fixed_length - ecg_signal.shape[1]\n",
    "            ecg_signal = np.pad(ecg_signal, ((0, 0), (0, pad_size)), 'constant')\n",
    "        else:\n",
    "            ecg_signal = ecg_signal[:, :self.fixed_length]\n",
    "        return ecg_signal\n",
    "\n",
    "# Пример использования\n",
    "# data = список из N записей, каждая содержит 8 каналов\n",
    "# labels = список меток заболеваний\n",
    "# dgen.init(filter=True)\n",
    "# amy, amyc, norm, amy_h, amyc_h, norm_h = dgen.read_data_with_meta()\n",
    "\n",
    "# X = norm + amyc + amy\n",
    "# y = np.concatenate([np.zeros(len(norm) + len(amyc)), np.ones(len(amy))])\n",
    "\n",
    "import pickle\n",
    "with open('../../Data/dumped/X_train.pkl', 'rb') as f:\n",
    "    f.seek(0)\n",
    "    X_train = pickle.load(f)\n",
    "with open('../../Data/dumped/y_train.pkl', 'rb') as f:\n",
    "    f.seek(0)\n",
    "    y_train = pickle.load(f)\n",
    "with open('../../Data/dumped/X_test.pkl', 'rb') as f:\n",
    "    f.seek(0)\n",
    "    X_test = pickle.load(f)\n",
    "with open('../../Data/dumped/y_test.pkl', 'rb') as f:\n",
    "    f.seek(0)\n",
    "    y_test = pickle.load(f)\n",
    "\n",
    "train_dataset = ECGDataset(data=X_train, labels=y_train[0])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = ECGDataset(data=X_test, labels=y_test[0])\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ECGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ECGNet, self).__init__()\n",
    "        \n",
    "        # Сверточные слои\n",
    "        self.conv1 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=7, padding=3)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5, padding=2)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # LSTM слой для захвата временных зависимостей\n",
    "        self.lstm = nn.LSTM(input_size=32, hidden_size=64, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Полносвязные слои\n",
    "        self.fc1 = nn.Linear(64*2, 128)\n",
    "        self.fc2 = nn.Linear(128, 3)  # Предполагается 3 класса болезней\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, 8, seq_len]\n",
    "        \n",
    "        # Свертка\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        \n",
    "        # Подготовка для LSTM\n",
    "        # Меняем размер на [batch_size, seq_len, channels] для LSTM\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # LSTM\n",
    "        x, (hn, cn) = self.lstm(x)\n",
    "        \n",
    "        # Берем последнее скрытое состояние LSTM\n",
    "        x = x[:, -1, :]  # [batch_size, 64*2]\n",
    "        \n",
    "        # Полносвязные слои\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # [batch_size, num_classes]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример обучения\n",
    "model = ECGNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=25):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in dataloader:\n",
    "            # Обнуление градиентов\n",
    "            optimizer.zero_grad()\n",
    "            inputs.to(device=\"cuda\")\n",
    "            labels.to(device='cuda')\n",
    "            # Прямой проход\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Обратный проход и оптимизация\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "# Запуск обучения\n",
    "# train_model(model, train_loader, criterion, optimizer, num_epochs=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiBranchECGNet(nn.Module):\n",
    "#     def __init__(self, num_channels=8, num_classes=3):\n",
    "#         super(MultiBranchECGNet, self).__init__()\n",
    "        \n",
    "#         # Создаем отдельную ветвь для каждого канала\n",
    "#         self.branches = nn.ModuleList([self.create_branch() for _ in range(num_channels)])\n",
    "        \n",
    "#         # Полносвязные слои после объединения всех ветвей\n",
    "#         self.fc1 = nn.Linear(1248, 128)\n",
    "#         self.fc2 = nn.Linear(128, num_classes)\n",
    "#         self.max_pool = nn.MaxPool2d(4)\n",
    "#         self.conv_1 = nn.Conv1d(256, 64, kernel_size=5, padding=2)\n",
    "#         self.conv_2 = nn.Conv1d(64, 32, kernel_size=5, padding=2)\n",
    "#         self.conv_3 = nn.Conv1d(32, 16, kernel_size=5, padding=2)\n",
    "#         self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    \n",
    "#     def create_branch(self):\n",
    "#         # Каждый канал проходит через свою сверточную ветвь\n",
    "#         branch = nn.Sequential(\n",
    "#             nn.Conv1d(1, 16, kernel_size=7, padding=3),  # Свертка с padding\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool1d(2),\n",
    "#             nn.Conv1d(16, 32, kernel_size=5, padding=2),  # Вторая с вертка\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool1d(2)\n",
    "#         )\n",
    "#         return branch\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         # x shape: [batch_size, num_channels, seq_len]\n",
    "        \n",
    "#         # Пропускаем каждый канал через свою ветвь\n",
    "#         branch_outputs = []\n",
    "#         for i in range(x.size(1)):  # num_channels\n",
    "#             branch_output = self.branches[i](x[:, i:i+1, :])  # Обрабатываем i-й канал\n",
    "#             branch_outputs.append(branch_output)\n",
    "        \n",
    "#         # Объединяем выходы всех ветвей\n",
    "#         out = torch.cat(branch_outputs, dim=1)  # Соединяем по оси каналов\n",
    "\n",
    "#         out = F.relu(self.conv_1(out))\n",
    "\n",
    "#         out = F.relu(self.conv_2(out))\n",
    "\n",
    "#         out = F.relu(self.conv_3(out))\n",
    "\n",
    "#         out = self.max_pool(out)\n",
    "        \n",
    "#         # Преобразуем для подачи на полносвязные слои\n",
    "#         out = out.view(out.size(0), -1)  # Flatten\n",
    "        \n",
    "#         # Полносвязные слои\n",
    "#         out = F.relu(self.dropout(self.fc1(out)))\n",
    "#         out = self.fc2(out)\n",
    "        \n",
    "#         return out\n",
    "\n",
    "# # Пример использования\n",
    "# model = MultiBranchECGNet(num_channels=8, num_classes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiBranchECGNet(nn.Module):\n",
    "    def __init__(self, num_channels=8, num_classes=3):\n",
    "        super(MultiBranchECGNet, self).__init__()\n",
    "        \n",
    "        # Ветви для каждого канала (CNN)\n",
    "        self.branches = nn.ModuleList([self.create_branch() for _ in range(num_channels)])\n",
    "        \n",
    "        # Attention слой для агрегации информации между каналами\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=128, num_heads=8, batch_first=True)\n",
    "        \n",
    "        # Полносвязные слои для классификации\n",
    "        self.fc1 = nn.Linear(128 * num_channels, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "    \n",
    "    def create_branch(self):\n",
    "        \"\"\"Создаем сверточную ветвь для каждого канала\"\"\"\n",
    "        branch = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=7, padding=3),  # Свертка с padding\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(32, 64, kernel_size=5, padding=2),  # Вторая сверточная операция\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),  # Третья сверточная операция\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        return branch\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, num_channels, seq_len]\n",
    "        \n",
    "        # Обрабатываем каждый канал через свою ветвь (CNN для каждого канала)\n",
    "        branch_outputs = []\n",
    "        for i in range(x.size(1)):  # num_channels\n",
    "            branch_output = self.branches[i](x[:, i:i+1, :])  # Обрабатываем i-й канал, [batch_size, 1, seq_len]\n",
    "            branch_outputs.append(branch_output)\n",
    "        \n",
    "        # Объединяем выходы ветвей\n",
    "        out = torch.stack(branch_outputs, dim=1)  # [batch_size, num_channels, 128, reduced_seq_len]\n",
    "        out = out.mean(dim=-1)  # Усредняем по временной оси: [batch_size, num_channels, 128]\n",
    "        \n",
    "        # Применяем multi-head attention для межканальной агрегации\n",
    "        out, _ = self.attention(out, out, out)  # [batch_size, num_channels, 128]\n",
    "        \n",
    "        # Flatten the output\n",
    "        out = torch.flatten(out, start_dim=1, end_dim=2)  # [batch_size, num_channels * 128]\n",
    "        \n",
    "        # Полносвязные слои для классификации\n",
    "        out = F.relu(self.drop(self.fc1(out)))\n",
    "        out = self.fc2(out)  # [batch_size, num_classes]\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Пример использования\n",
    "model = MultiBranchECGNet(num_channels=8, num_classes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 0.6325\n",
      "Epoch 2/25, Loss: 0.5698\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Запуск обучения\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 14\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     16\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32md:\\Projects\\Diplomas\\venv\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Projects\\Diplomas\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Projects\\Diplomas\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Обучение модели\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "# Запуск обучения\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1]\n",
      "[1 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 0 0 1 1 0 0 1 0 0]\n",
      "Validation recall: 0.5000\n",
      "Validation accuracy: 0.4000\n",
      "Validation precision: 0.4000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, accuracy_score, precision_score\n",
    "\n",
    "def validate_model(model, dataloader):\n",
    "    model.eval()  # Переводим модель в режим оценки\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():  # Отключаем градиенты для валидации\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)  # Предсказания с максимальной вероятностью\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    # Преобразуем в numpy массивы\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    print(all_preds)\n",
    "    print(all_labels)\n",
    "    \n",
    "    # Считаем accuracy\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    print(f'Validation recall: {recall:.4f}')\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f'Validation accuracy: {accuracy:.4f}')\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    print(f'Validation precision: {precision:.4f}')\n",
    "    \n",
    "    return recall\n",
    "\n",
    "# Пример вызова валидации\n",
    "test_accuracy = validate_model(model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
